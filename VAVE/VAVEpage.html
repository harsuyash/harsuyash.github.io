<!doctype html>
<html>

<head> <title> Har Suyash </title>
        
	<!-- RESPONSIVE -->
	<meta name="viewport" content="width= device-width">
        
    <!-- FAVICON -->    
    <link rel="icon" type="image/png" href="../Homepage/favicon.png" >
        
    <!-- FONTS -->    
	<link href="https://fonts.googleapis.com/css2?family=Libre+Franklin:ital,wght@0,400;0,600;0,700;1,400;1,600;1,700&family=Merriweather:ital,wght@0,300;0,400;0,700;0,900;1,300;1,400;1,700;1,900&display=swap" rel="stylesheet">
     <link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Serif:wght@400;500;600&family=Noto+Serif:wght@400;700&family=PT+Serif:wght@400;700&display=swap" rel="stylesheet">
    
    <!-- STYLESHEET -->    
    <link href="vave.css" rel="stylesheet" type="text/css">
   
    
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    
    
        
</head>


    
<body>
    
    
     
    <header class="menu" id="myHeader">
            <div class="topnav" id="mynav">
                <a href="../index.html"><img src="../Homepage/HSB%20LOGO.svg" alt="Logo" width="32px" class="homeicon" id="hidden" style="margin-left: auto; margin-right: auto; margin-top: 7px"></a>
                    <div class="topnav-right">
                        
                         
                         <a href="../index.html#anchor-mywork" style="margin-top: 10px">Case Studies</a>
                         <a href="../Gallery/Gallery.html"  style="margin-top: 10px; margin-left: 3px"> Gallery</a>
                         <a href="/Homepage/Resume.pdf" class="resume"> <img src="../Homepage/download.svg"  class="download" alt="instagram account" width="18px" style="margin-top: 7px"> Resume</a>
                       
                </div>
            </div>
    </header>
    
    
    <!-- COVER PHOTO -->
    <div class="coverpage"> 
         <p class="title" align="center" style="margin-top: 60px;">VAVE</p>
         <p class="headings" style="text-align: center; margin-top: 0px;color: #777777">Gesture based musical instrument</p>
        
         </div> 
    
     <div class="row" style="max-width: 790px; margin-top: 30px; margin-bottom: 100px;">
                <div class="column" style="min-width: 40%; margin-right: 48px;">
                    <p class="headings" style="margin-top: 20px">Problems addressed</p>
            <p class="content">Exploring different ways to interact in mixed reality</p>
                </div>
                
                <div class="column" style="min-width: 41%;">
                    <p class="headings" style="margin-top: 20px">Key Contribution</p>
                    <p class="content"> Using sensors that record hand gestures for playing music </p>
                </div>
            </div>
    
    
    
      <p align="center" class="fields" style="margin-bottom: 30px; margin-top: 60px; font-size: 18px; font-style: italic; font-family: 'IBM Plex Serif, serif">Scroll to see the whole journey below </p>
    
  
    
    
    <nav class="progress" id="myHeader2">
            <ul class="topprog" id="myprog">
                
                 <li ><a href="../POUR/POURpage.html" class="next-marker" style="border: 0; padding-left: 12px" ><img src="../chevrons-left.svg" alt="previous project" width="18px" style="float: left; padding-top: 2px" ></a></li>
                <li ><a href="#intro" class="intro-marker active " >INTRODUCTION &emsp; ></a> </li> 
                <li ><a href="#why" class="why-marker " > INSPIRATION &emsp;></a></li> 
                <li ><a href="#define" class="define-marker " > EXPLORATION &emsp;></a></li>
                <li ><a href="#inception" class="inception-marker " > NARROWING DOWN &emsp;></a></li>
                <li ><a href="#validate" class="val-marker " > FINAL PROTOTYPE AND TESTING &emsp;></a></li>
                <li ><a href="#conc" class="conc-marker " > CONCLUSION &emsp;></a></li>
                <li ><a href="../ARCAR/ARCARpage.html" class="next-marker" style="margin-right: -20px">NEXT</a></li>
                <li ><a href="../ARCAR/ARCARpage.html" class="next-marker" style="border-bottom:0"><img src="../chevrons-right.svg" alt="previous project" width="18px" style="float: left; padding-top: 1px" ></a></li>
              
            </ul>
           </nav>
    
    
    <!-- COVER PHOTO -->
   <img src="VAVE.png" width="100%">  

    
    
    <div style="background-color: rgba(230, 230, 230, 1); width: 100%; padding-bottom: 100px; padding-top: 100px; margin-top: 0;">
    
    
    
    <!-- INTRODUCTION -->
<section class="intro" id="intro">
<p class="sections">INTRODUCTION</p>
<div class="cards">   
    
 
    <p class="headings">Project Background</p>
    <p class="content">Vave, a 3-week long project in collaboration with Ameya Nikose, is an exploration of mixed media interaction techniques by creating a virtual gesture-based instrument. The project’s motivation was to learn to work with augmented/virtual/mixed reality and create an enjoyable musical experience. </p>
    
    
    <p class="headings">Create an enjoyable musical experience while exploring interactions in augmented/virtual reality.</p>
    
    <p class="headings">Final Output</p>
    <p class="content">The final product was a handy instrument played by waving and making other hand gestures over it. The physical instrument was to have a tangible mode of interaction. It was finally interfaced using Unity with visualizations that could be seen in VR for visual feedback. </p>
    
   <center><iframe src="https://drive.google.com/file/d/18v7-rPp1YktBfFcmnURCcBRFkDePWRsZ/preview" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen style="margin-top: 30px; max-width: 640px; height: 640vh;  "></iframe></center>
    
    </div>     
</section> <!-- INTRODUCTION -->
    
    
    
    

        
        
   <!-- INSPPIRE -->
<section class="why" id="why">
<p class="sections">INSPIRATION</p>
<div class="cards">   
    
    
    
    
    <p class="headings">An Instrument or a Game</p>
    <p class="content">To create an enjoyable music experience, we started by learning about recent technologically driven musical instruments, games, and experiences. The first challenge that we faced was to define whether we were going to create an instrument or a game. We looked at Beat-Saber ( an existing musical game in virtual reality). Initially, we came up with some ideas on whether to carry forward with creating an instrument or a game. </p>
    
    <center>
       <img src="ideation.png" style="width: 75%; margin-top: 30px" class="img">
    </center>
   
    <p class="content">After speculating a bit about the idea of an instrument, we selected the concept of creating an instrument.</p>
    
    
     <p class="headings">Inspiration for creating an instrument</p>
    <p class="content">To create an enjoyable experience, we again dived deeper and looked for existing technical music instruments and bodily experiences to create music.
The inspirations for digital musical instruments were of theremin. We also got inspired by fiction like Mr. Bean for the gestures and emotions in playing music.</p>
	<center>
       <img src="Inspiration.png" style="max-width: 640px; margin-top: 30px" class="img">
    </center>
    
       
    
    <p class="headings">Theremin- Playing Music in Thin Air</p>
    <p class="content">The theremin is an electronic musical instrument controlled without physical contact by the thereminist (performer). The instrument's controlling section usually consists of two metal antennas that sense the relative position of the thereminist's hands and control oscillators for frequency with one hand and amplitude (volume) with the other. The electric signals from the theremin are amplified and sent to a loudspeaker. </p>
     
    <p class="headings">Using Hand Movements in Mixed Reality</p>
    <p class="content">The video from Yago De Quay about Augmented Reality Musical Instrument has a very interesting concept of controlling the song/music parameters by hand movements using motion capture technology.
</p>
   
    <p class="headings">Musical Installation </p>
    <p class="content">We also looked at different experiences like piano-tap-dance, where the players played a huge piano by dancing on the notes to play it.
</p>
    
    <p class="headings">Mr. Bean- Gesture and Emotion Driven</p>
    <p class="content">Finally, we had an idea of creating a musical instrument that is solely gesture and emotion-driven. This can be explained by this video of Mr. Bean, where he uses emotions to drive and conduct the choir. Wouldn’t it be awesome to play an instrument like that someday!
</p>
       

    
</div>     
</section> <!-- INSPIRE -->
    
    
 
        
        
        
        
        
        
        
        
    
    
       <!-- EXPLORE -->
<section class="define" id="define">
<p class="sections">EXPLORATION</p>
<div class="cards">   
    
    
    
     <p class="headings">Exploration for Creating an Instrument</p>
    <p class="content">After getting inspired by looking at some wild things on the internet, we decided to also look into the technical part of the process and explore various media. So we started exploring different ways of building the instrument simultaneously.
    <br><br>
        We had a lot of fun trying to learn about and quickly prototype different interaction techniques in AR/VR/MR. As we did not have access to existing controllers, we had to look at other alternatives.
    </p>
    
    <p class="headings">Figuring Out Ways to Interact</p>
    <p class="content">We looked at methods to interact without a controller, like using a raycast reticle to point and shoot using a phone, using virtual buttons on an image marker in AR, and using onscreen buttons in AR. </p>
    
     <p class="headings">VR Gaze</p>
    <p class="content">One simple way to interact in VR was by using the accelerometer of the phone and the Raycast Reticle feature in Google VR SDK that tracks where the person is looking/ gazing and triggers a pre-defined interaction with that element. Here in our prototype, when a person gazes at a cube, the cube changes its color from white to red and if the person continues to stare for a specified time limit, it changes to black.   </p>
      <center>
       <img src="vr%20gaze.png" style="width: 90%; margin-top: 0px" class="img">
    </center>
    <p class="caption" style="margin-top: 0">Learnings: A simple inbuilt interaction but moving around a lot to point the phone at elements in space could be tiring. </p>
    
     <p class="headings">On-screen buttons in AR</p>
    <p class="content">In our next prototype, some keys pop up in the AR space and when these are pressed on the screen, a song is played. </p>
     <center><iframe src="https://drive.google.com/file/d/1fx-3wincCcgdfvDlPqSjsnkQ02AYKKwp/preview" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen style="margin-top: 30px; max-width: 640px; height: 640vh; "></iframe></center>
    <p class="caption">Learnings: Although in AR, the experience was like any other 2D application and not enjoyable.  </p>
    
    
    
    <p class="headings">Mixed-Reality and Virtual Buttons</p>
    <p class="content">We also prototyped wearable AR by making a stereoscopic AR camera in Unity. This helped in having a tracker-based AR with virtual buttons, at the same time keeping your hands free, unlike hand-held AR using a mobile device. As the hands were free, we could add some virtual buttons in the 3D world space that could be interacted with using our hands. </p>
    <center><iframe src="https://drive.google.com/file/d/154RvjL8lNREDM1RT0pnAdzT1Bdc9EWqF/preview" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen style="margin-top: 30px; max-width: 640px; height: 640vh; "></iframe></center>
    <p class="caption">Learnings: Interactions became more immersive but the virtual buttons only work when visible through the camera. </p>
    
    <p class="headings">Making Our Own Controllers</p>
    <p class="content">We also tried to make our own controllers with simple sensors like accelerometer, gyroscope, and ultrasonic sensor and map hand movements recorded through them to different musical variables. </p>
    <center><iframe  src="https://drive.google.com/file/d/1Al2TcqojT00nYo6-iDlEv4CtiAUncrU-/preview" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen style="margin-top: 30px; max-width: 640px; height: 640vh; "></iframe></center>
    <p class="caption">Learnings: As the sensors were independent from AR camera, one could look anywhere while interacting with the controllers. 
    <br><br>
    This also enabled a more tangible mode of interacting and we decided to go further with this.</p>
    
    <p class="headings">But Where Is The Music In All This? - Audio Helm</p>
    <p class="content">Another challenge was to produce musical sounds in Unity based on the feedback from sensors/ virtual buttons/ raycast. We used the Audio Helm plugin, which is a live audio synthesizer, sequencer, and sampler for Unity that gives you the tools to create dynamic sound effects and generative music. We then tweaked the code to control audio helm variables according to the input from sensors to make it interactive with&nbsp;gestures.</p>
    <center>
       <img src="AudioHelm.png" style="max-width: 640px; margin-top: 30px" class="img">
    </center>
    
    
</div>        
</section> <!-- EXPLORE -->
   
 
        
 
        
        
        
        
        
        
        
        
    
  <!-- Narrowing Down -->
<section class="inception" id="inception">
<p class="sections">NARROWING DOWN</p>
<div class="cards">   
    
    
       <p class="headings">Gestures for Playing Music</p>
    <p class="content">For a holistic seamless experience, we not only had to figure out the technical aspects but also think about the usability and experience design aspects. In order to decide on the gestures that will be intuitive and easy to perform to control the music, we did some role play and act-it-out by simply doing actions that pretend to control existing music. These wizard of oz prototypes helped understand the different hand movements that are enjoyable and natural for controlling different music variables like pitch, volume, sustain, vibrato. We also took inspiration from the Mr. Bean video.</p>
     <center>
       <img src="how%20to%20play.jpg" style="width: 100%; margin-top: 30px" class="img">
    </center>
    
    
    <p class="headings">Implementation- Wearable Controller</p>
    <p class="content">For connecting the intended design with the technology, we had to figure out the implementation for the different gestures. As most of the gestures were free-flowing hand gestures, we decided to go with a wearable glove with sensors. A 6 DOF accelerometer was used to get the rotational and transnational values of the hand movement. We added some modalities with buttons on the fingers. One could pinch with a thumb and index finger and move the hand as if holding and moving a virtual slider to control any variable.</p> 
     <center>
       <img src="wearable.jpg" style="width: 100%; margin-top: 30px" class="img">
    </center>
    
    <p class="headings">Discrete vs Continuous Music</p>
    <p class="content">One of the decisions that we had to make was whether to put a smooth transition between two semitones (like in a theremin) or to jump directly between semitones. Finally, after testing both the scenarios, we decided to put just the semitones as the notes between two semitones were difficult to play.</p>
    
  
    
    
</div>        
</section> <!-- PROTO -->
        
   
        
  
       
        
        
        
        
        
        
              
    <!-- TEST -->
<section class="val" id="validate">
<p class="sections">PROTOTYPE AND TESTING</p>
<div class="cards">   
    
    
    <p class="headings">Tangible Instrument for Virtual Reality</p>
    <p class="content">On our concept so far, we got feedback on including something tangible to interact with. Having intimacy and interacting with another instrument makes the experience of playing music more enjoyable and also something that we are used to. Considering this, we designed a simple wave-like form that is coherent with the kind of interaction we want. It’s a light instrument that can be held with one arm like a child and played with by waving the other hand. It contains an ultrasonic sensor at one end, and its readings have been calibrated to play different notes of a scale on each crest and trough of the form. For the prototype, the instrument contains one octave of G Major scale.</p>
    <center>
       <img src="Views.png" style="width: 100%; margin-top: 30px" class="img">
    </center>
    
     <p class="headings">Crests and Troughs for Tactile Feedback</p>
     <center>
       <img src="vave.18.jpg" style="width: 100%; margin-top: 30px" class="img">
    </center>
    
     <p class="headings">Curves to Easily Fit in Hand</p>
    <center>
       <img src="vave.3.jpg" style="width: 100%; margin-top: 30px" class="img">
    </center>
    
     <p class="headings">Physical Prototype and Role Playing</p>
    <center>
       <img src="Concept.jpg" style="width: 100%; margin-top: 30px" class="img">
    </center>
    
    <p class="headings">Back end- sensor, Arduino, and interfacing in Unity</p>
    <p class="content">The main component of the instrument was the ultrasonic sensor that measured the distance of the hand from one end of the instrument. This was recorded using an Arduino Uno, and the Arduino data was sent to Unity using serial port communication. In Unity, we used C# and Audio Helm to play a musical note based on the position of the hand. Another component was an accelerometer that recorded the angle of the hand to control the volume (flatten the hand on the instrument to lower volume and raise upright to increase.) The accelerometer also measured the vibrations of the hand at a position to play vibrato. We tried to optimize the code by setting an optimum range of the sensor and delays in sending data to Unity, so we don’t overwhelm unity with a lot of data creating a lag at the same time sending enough data to keep it precise to the hand movements. However, there was still some amount of lag with all the sensors and data coming in, so we decided to just go with the ultrasonic sensor for our MVP to play different notes with the hand movements.</p> 
    <center>
       <img src="tools%202.jpg" style="width: 100%; margin-top: 30px" class="img">
    </center>
    
    <p class="headings">Virtual Landscape</p>
    <p class="content">We also needed some visual feedback for a complete experience. All this was interfaced using Unity itself. We used a sphere in the world space in VR that moves with the movement of the hand (the position of the sphere corresponds to the data from the ultrasonic sensor). On the path of the sphere, we laid some bars. When the sphere collides with a bar, it glows/ ripples, and a musical note corresponding to the bar is played using Audio Helm. For the working prototype, we kept it simple, but we also created richer visualizations that could enhance the experience of playing the instruments. 
<br><br>
As the instrument is in VR, we decided to propose the possibility of a collaborative virtual environment where different people can come together for jamming. “Near far wherever you are, I believe in my VAVE, the show must go on and on…” 
</p><center>
       <img src="VLandscape.png" style="max-width: 640px; margin-top: 30px" class="img">
    </center>
    
    <p class="headings">Final Prototype</p>
    <center><iframe  src="https://drive.google.com/file/d/18v7-rPp1YktBfFcmnURCcBRFkDePWRsZ/preview" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen style="margin-top: 30px; max-width: 640px; height: 640vh; "></iframe></center>
    
    
    <p class="headings">Testing- Given To Use Without Instructions</p>
    <center><iframe src="https://drive.google.com/file/d/11T-6S9oQnwSBzFLzZ4PeYjQeNAxBKRUw/preview" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen style="margin-top: 30px; max-width: 640px; height: 640vh; "></iframe></center>
    
   
    
</div>         
</section> <!-- TEST -->
        
       
    
        
        
        
        
        
        
        
        
        
    <!-- CONCLUSION -->
<section class="conc" id="conc">
<p class="sections">CONCLUSION</p>
<div class="cards">   
    
    
    <p class="headings">Feedback and Future Scope</p>
    <p class="content">The experience of creating music was enjoyable indeed. People were fascinated by being able to create music by vaving hand in the air. The gestures were also percieved as intuitive, natural and enjoyable. We had not instructed the users about how to play the instrument as we wanted to see if they are able to figure it out. The users (who were familiar with playing some instrument and how to play music) were able to figure out the interactions on their own. Some scope for improvement regarding the instrument was regarding the scale and limited octave. There had to be ways to caliberate for different scales and extend the octave. One option could be to move between octaves similar to applications like perfect piano i.e. by sliding but that could hamper the experience. Another option could be to have interactive area even outside the physical instruement.</p>
    
    <p class="headings">Learnings</p>
    <p class="content">Through the project we learnt technical skills like using arduino, sensors, Unity, etc. We also learnt design sills like how to quickly prototype and test ideas or get expert 
Feedback. It was also exciting to think about the tangible interactions and gestures to control musical variables.
</p>
   
    
    
</div>         
</section> <!-- CONCLUSION -->    
    
  
    
  
</div>
    
    
    
           <section class="next">
       
       
        <div id="anchor-mywork" style="width:100%; padding-bottom: 150px; background: linear-gradient(190deg, rgba(241,249,255,1) 30%, rgba(253,245,248,1) 60%, rgba(249,244,243,1) 100%);">
                <img src="../Homepage/Down%20Arrow.png" alt="Logo" width="140px" style="display: block; margin-left: auto; margin-right: auto;">
                <p class="headings" style="color: #777777">Check Out More</p>
                <div class="projects">
                    <div id="ARCar">
                        <a href="../ARCAR/ARCARpage.html"><img src="../Homepage/AR%20CAR.png" alt="A R car" class="images" >
                       <p class="headings2" align="left" style="color: #519FCC">Interactions in Augmented Reality</p></a> 
                        <p align="left" class="content2">The project explored the topic of How Stuff Works using Augmented Reality as a medium. For the scope of this project, I explain how an IITB Racing car works, through a hand held device AR. Looked at key interaction design problems. </p> 
                        <p align="left" class="fields"> MAY 2020 | &emsp; AR design  &emsp;  Interaction design  &emsp;  Unity prototyping</p> 
                    </div>
                     <div id="POUR" style=" margin-top: 150px">
                        <a href="../POUR/POURpage.html"><img src="../Homepage/POUR.png" alt="POUR"  class="images">
                            <p align="left" class="headings2" style="color: #dd5588">Assistive Device for Blind Users </p> </a>
                        <p align="left" class="content2">Designed an assistive device for blind users to help safely pour hot liquids. The project explored cognitive ergonomics and how the form of the product could communicate useful information,  providing a feedback loop for the blind. </p> 
                        <p align="left" class="fields">FEB 2020 | &emsp; Product design  &emsp;  Tactile interactions &emsp; User study</p> 
                    </div>
                    
                   <div id="PaperPotli" style=" margin-top: 150px">
                       <a href="../PaperPotli/PaperPotlipage.html"> <img src="../Homepage/Paper%20Potli.png" alt="Paper Potli"  class="images">
                        <p align="left" class="headings2" style="color: #E79C46">Game for Gender Equality </p> </a> 
                        <p align="left" class="content2">A craft + AR avatar making game for making children gender aware. Designed an end to end model for promoting gender equality among children through craft workshops and a digital application. The DIY dolls help explain, express and explore gender.</p> 
                        <p align="left" class="fields">OCT 2019 | &emsp; Game design  &emsp;  Social design &emsp; User study</p> 
                    </div>
                    
                    
                   
                    
                   <!-- <div id="VAVE" style=" margin-top: 150px">
                        <img src="../Homepage/VAVE.png" alt="VAVE"  class="images">
                        <p align="left" class="headings2" style="color:#617C8D">Virtual Music Instrument</p> 
                        <p align="left" class="content2">Designed and prototyped a virtual music instrument played with hand gestures. Explores tangible interactions to map different music variables with intuitive hand gestures. Explored mixed reality to visualize music.</p> 
                        <p align="left" class="fields">JAN 2020 | &emsp; Interaction design  &emsp;  Tangible interaction &emsp; Mixed reality</p> 
                    </div>-->
                    
                   
            
            
                </div>
           </div>
            <div id="mycontact"  style=" padding-top: 50px; padding-bottom: 50px;"><p align="center" class="content">Thanks for visiting!</p></div>
       
       
       </section>
    
    
    
    
    
    
    
    
   <script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.3/jquery.min.js"></script>
		<script type="text/javascript">
    
    
window.onscroll = function() {myFunction(); myFunction2(); myFunction3(); myFunction4()};

var header = document.getElementById("myHeader");
var sticky = header.offsetTop;

function myFunction() {
  if (window.pageYOffset > sticky) {
    header.classList.add("sticky");
  } else {
    header.classList.remove("sticky");
  }
}

var header2 = document.getElementById("myHeader2");
var sticky2 = header2.offsetTop;

function myFunction2() {
  if (window.pageYOffset > sticky2) {
    header2.classList.add("sticky2");
  } else {
    header2.classList.remove("sticky2");
  }
}           
            
/* When the user scrolls down, hide the navbar. When the user scrolls up, show the navbar */
var prevScrollpos = window.pageYOffset;
function myFunction3() {
  var currentScrollPos = window.pageYOffset;
  if (prevScrollpos > currentScrollPos) {
    document.getElementById("myHeader").style.top = "0";
  } else {
    document.getElementById("myHeader").style.top = "-50px";
  }
  prevScrollpos = currentScrollPos;
}

var prevScrollpos2 = window.pageYOffset;
function myFunction4() {
  var currentScrollPos2 = window.pageYOffset;
  if (prevScrollpos2 > currentScrollPos2) {
    document.getElementById("myHeader2").style.top = "60px";
  } else {
    document.getElementById("myHeader2").style.top = "0px";
  }
  prevScrollpos2 = currentScrollPos2;
}
			
            
            
            (function() {

                var navLinks = $('.topprog li a'),
                    navH = $('.progress').height(),
                    section = $('section'),
                    documentEl = $(document);
                
                documentEl.on('scroll', function() {
                    var currentScrollPos = documentEl.scrollTop();
                    
                    section.each(function() {
                        var self = $(this);
                        if ( self.offset().top < (currentScrollPos + navH) && (currentScrollPos + navH) < (self.offset().top + self.outerHeight()) ) {
                            var targetClass = '.' + self.attr('class') + '-marker';
                            navLinks.removeClass('active');
                            $(targetClass).addClass('active');
                        }
                    });
                    
                });
        })();
    

    
</script>

    </body> 
    
</html>
